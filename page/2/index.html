<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="referrer" content="no-referrer" />
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/img/title.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/img/title.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/img/title.ico">
  <link rel="mask-icon" href="/img/title.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.31/dist/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"qingy735.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":false,"copycode":{"enable":true,"show_result":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="QingY&#39;s Code Space">
<meta property="og:url" content="https://qingy735.github.io/page/2/index.html">
<meta property="og:site_name" content="QingY&#39;s Code Space">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="QingY">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://qingy735.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>QingY's Code Space</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">QingY's Code Space</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="heartbeat fa-fw"></i>公益 404</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="QingY"
      src="/img/title.ico">
  <p class="site-author-name" itemprop="name">QingY</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/qingy735" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qingy735" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:qingy735@qq.com" title="E-Mail → mailto:qingy735@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://qingy735.github.io/posts/2293.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/title.ico">
      <meta itemprop="name" content="QingY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QingY's Code Space">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | QingY's Code Space">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/2293.html" class="post-title-link" itemprop="url">《A Deeply Supervised Attention Metric-Based Network and an Open Aerial Image Dataset for Remote Sensing Change Detection》笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-11-23 10:24:44" itemprop="dateCreated datePublished" datetime="2022-11-23T10:24:44+08:00">2022-11-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 20:14:36" itemprop="dateModified" datetime="2024-04-03T20:14:36+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>论文地址：<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9467555">A Deeply Supervised Attention Metric-Based Network and an Open Aerial Image Dataset for Remote Sensing Change Detection</a></p>
</blockquote>
<h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2>
<h3 id="动机"><a class="markdownIt-Anchor" href="#动机"></a> 动机</h3>
<p>尽管近几年深度学习方法在变化检测中取得了很多实质性突破，但是<strong>变化检测的结果容易受到光照、噪声、和尺度等外部因素的影响，从而导致检测图中出现伪变化和噪声</strong></p>
<h2 id="主要工作"><a class="markdownIt-Anchor" href="#主要工作"></a> 主要工作</h2>
<ul>
<li>
<p>提出了一种基于 <strong>深度监督（DS）</strong> 注意力度量的网络（DSAMNet）</p>
</li>
<li>
<p>集成CBAM模块，在空间和通道方面获得更具有辨别力的特征；使用深度监督以实现更好的特征提取</p>
</li>
<li>
<p>提出了一个新的变化检测数据集SYSU-CD，并且模型在CDD和SYSU-CD上取得了很好的效果</p>
</li>
</ul>
<h2 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h2>
<p>DSAMNet整体结构图如下所示，主要可以分为三部分：特征提取、度量学习、深度监督。特征提取部分主要采用ResNet网络进行多尺度多深度特征的提取；度量学习部分首先将输入特征经过CBAM模块处理得到更具有辨别性的特征，然后计算双时相特征之间的距离，最后根据<strong>BCL Loss</strong>进行优化；深度监督部分主要由一个深度监督层和损失计算构成，深度监督层负责将特征上采样到输入图像大小。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123103155809.png" alt="image-20221123103155809" /></p>
<h3 id="cbam"><a class="markdownIt-Anchor" href="#cbam"></a> CBAM</h3>
<p>CBAM模块主要进行通道注意力和空间注意力处理，空间注意力和通道注意力均采用最大池化和平均池化处理，不过一个在空间维度，另一个在通道维度。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123104338589.png" alt="image-20221123104338589" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123104548679.png" alt="image-20221123104548679" /></p>
<h3 id="度量学习模块"><a class="markdownIt-Anchor" href="#度量学习模块"></a> 度量学习模块</h3>
<p><strong>为了充分利用低层特征中丰富的空间信息和高层特征中的语义信息</strong>，首先将ResNet提取到的每一层特征进行通道统一（全部变成96，并且统一尺寸为输入图像的一半），然后将每一层的特征进行concat后输入到CBAM模块计算双时相特征之间的距离，最后和GT进行计算BCL Loss。其中BCL Loss计算如下图所示</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123104632659.png" alt="image-20221123104632659" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123110522900.png" alt="image-20221123110522900" /></p>
<h3 id="深度监督模块"><a class="markdownIt-Anchor" href="#深度监督模块"></a> 深度监督模块</h3>
<p>由于网络的中间层的训练是不透明的而且缺乏监督，所以中间层并不能很好地学习有效特征，所以使用深度监督模块来增强浅层特征的表征能力。主要针对前两层特征进行深度监督处理，文章中提到这两层同时具备丰富的空间信息和语义信息。在输出变化图后与GT进行Dice Loss计算来优化网络。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123121048547.png" alt="image-20221123121048547" /></p>
<h2 id="实验"><a class="markdownIt-Anchor" href="#实验"></a> 实验</h2>
<h3 id="对比实验"><a class="markdownIt-Anchor" href="#对比实验"></a> 对比实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123122838424.png" alt="image-20221123122838424" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123122847259.png" alt="image-20221123122847259" /></p>
<h3 id="可视化对比实验"><a class="markdownIt-Anchor" href="#可视化对比实验"></a> 可视化对比实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123122911508.png" alt="image-20221123122911508" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123122929541.png" alt="image-20221123122929541" /></p>
<h3 id="消融实验"><a class="markdownIt-Anchor" href="#消融实验"></a> 消融实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123123000141.png" alt="image-20221123123000141" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123123021835.png" alt="image-20221123123021835" /></p>
<h3 id="超参lambda实验"><a class="markdownIt-Anchor" href="#超参lambda实验"></a> 超参<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123123122491.png" alt="image-20221123123122491" /></p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<ul>
<li>
<p>本文提出了一种新的深度学习处理变化检测的方法：使用CBAM模块直接从特征中学习到变化图，同时使用辅助深度监督模块。用于生成具有更多空间信息的变化图。</p>
</li>
<li>
<p>CBAM可以有效地使特征更具有判别性，从而辅助度量模块的学习；同时深度监督模块可以很好地利用中间块特征中包含的信息，从而进一步改进度量模块学习到的变化图。</p>
</li>
<li>
<p>通过在CDD、SYSU-CD数据集上实验表示该方法结果优于其他最先进的方法。</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://qingy735.github.io/posts/49056.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/title.ico">
      <meta itemprop="name" content="QingY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QingY's Code Space">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | QingY's Code Space">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/49056.html" class="post-title-link" itemprop="url">《Optical Remote Sensing Image Change Detection Based on Attention Mechanism and Image Difference》笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-11-21 10:41:12" itemprop="dateCreated datePublished" datetime="2022-11-21T10:41:12+08:00">2022-11-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 20:14:36" itemprop="dateModified" datetime="2024-04-03T20:14:36+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>论文地址：<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9254128">Optical Remote Sensing Image Change Detection Based on Attention Mechanism and Image Difference</a></p>
</blockquote>
<h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2>
<h3 id="动机"><a class="markdownIt-Anchor" href="#动机"></a> 动机</h3>
<p><strong>为了建模高层和低层特征之间的内在相关性</strong></p>
<p>提出了一种由多个上采样注意力单元组成的密集注意力方法。该单元同时采用了上采样空间注意力和上采样通道注意力。该单元可以利用具有丰富类别信息的高层特征来指导低层特征的选择，可以利用空间上下文信息来捕捉地物变化的特征。而且还引入差异增强模块促使特征选择性地聚合。</p>
<h2 id="主要工作"><a class="markdownIt-Anchor" href="#主要工作"></a> 主要工作</h2>
<ul>
<li>提出一种端到端的网络架构用于变化检测</li>
<li>提出了一种密切关注特征融合每个阶段的稠密注意力方法。在每个上采样注意力（UA）单元中，采用上采样空间注意力（SA）和通道注意力（CA）来同时捕捉空间和通道维度的变化信息。<strong>UA单元可以很好地定位细节信息和纹理特征</strong></li>
<li>提出差异增强模块，可以直接将差异图像直接映射到新的特征空间，充分挖掘变化信息，生成变化稠密图</li>
</ul>
<h2 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h2>
<p>该方法整体结构与U-Net类似，采用U-Net和DenseNet结合的方式，多的部分为上面一个分支：差异增强模块（该作者似乎很看好这个方法，在<a target="_blank" rel="noopener" href="https://qingy735.gitee.io/posts/28607.html">TransUNetCD</a>这篇论文中也是使用了相同的增强方法，具体介绍可以阅读文章）。在图像输入之初便将双时相图像进行concat处理，然后再Decoder部分进行<strong>UA</strong>模块处理，最后和差异增强模块部分的特征进行相乘得到最终的差异图。</p>
<p>作者解释在解码器中使用稠密注意力机制是因为可以更好地保留双时相遥感图像中变化区域的纹理和细节信息，不仅增加了抗干扰能力，而且提高了检测小物体变化的能力。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121175405265.png" alt="image-20221121175405265" /></p>
<h3 id="ua模块"><a class="markdownIt-Anchor" href="#ua模块"></a> UA模块</h3>
<p>论文中提到：高级特征具有丰富的语义类别信息，能够很好地为低级特征类别定位作指导。同时高级特征与用于对低级特征进行加权来获取精确的像素级合并。其中UA模块结构如下图所示。首先看左边，高级特征上采样后与低级特征进行<strong>add</strong>后经过一个卷积层输出空间维度的掩码 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>s</mi><msub><mi>k</mi><mrow><mi>S</mi><mi>A</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mi>W</mi><mo separator="true">,</mo><mi>H</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">mask_{SA}(1,W,H)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mord mathnormal mtight">A</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mclose">)</span></span></span></span> ，之后与低级特征进行空间维度相乘得到经过空间注意力处理的特征；其次是右边，高级特征首先进行通道注意力处理，然后经过一个卷积层输出通道维度的掩码 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>s</mi><msub><mi>k</mi><mrow><mi>C</mi><mi>A</mi></mrow></msub><mo stretchy="false">(</mo><mi>C</mi><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">mask_{CA}(C,1,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mord mathnormal mtight">A</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span> ，最后和上面的特征进行相乘；最后将上采样后的高级特征与上述输出进行concat处理，后得到最终输出。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121180719472.png" alt="image-20221121180719472" /></p>
<blockquote>
<p>注：以下为个人理解</p>
</blockquote>
<p>首先分析空间注意力部分：因为高级特征具有很丰富的语义信息，所以在上采样后与低级特征进行add操作是为了指导变化物体的位置信息，最后输出空间维度掩码指导低级特征是为了更好地定位变化目标。</p>
<p>而通道注意力部分：由于高级特征具有很好地语义信息，所以不同通道中可以很好地识别出背景和前景，于是进行通道注意力处理再与浅层特征相乘有利于区分变化区域和非变化区域（这里指背景）。</p>
<p>而为什么最后还要进行concat操作，是因为高级特征包含信息很丰富，上述操作只是利用了一部分，进行cancat操作可以更好地利用高级特征信息。</p>
<h3 id="de模块"><a class="markdownIt-Anchor" href="#de模块"></a> DE模块</h3>
<p>只是简单的求差分之后残差卷积处理，个人认为会包含很多无关变化（受光照、拍摄角度、季节等影响）。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183042531.png" alt="image-20221121183042531" /></p>
<h2 id="实验"><a class="markdownIt-Anchor" href="#实验"></a> 实验</h2>
<h3 id="消融实验"><a class="markdownIt-Anchor" href="#消融实验"></a> 消融实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183508942.png" alt="image-20221121183508942" /></p>
<h3 id="对比实验"><a class="markdownIt-Anchor" href="#对比实验"></a> 对比实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183522653.png" alt="image-20221121183522653" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183603827.png" alt="image-20221121183603827" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183622304.png" alt="image-20221121183622304" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183758543.png" alt="image-20221121183758543" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183731917.png" alt="image-20221121183731917" /></p>
<h3 id="可视化实验"><a class="markdownIt-Anchor" href="#可视化实验"></a> 可视化实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183639818.png" alt="image-20221121183639818" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183658373.png" alt="image-20221121183658373" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183816898.png" alt="image-20221121183816898" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121183834453.png" alt="image-20221121183834453" /></p>
<blockquote>
<p>注：原论文包含更详细的介绍，如果感兴趣可以阅读原文，链接见上方</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://qingy735.github.io/posts/41400.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/title.ico">
      <meta itemprop="name" content="QingY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QingY's Code Space">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | QingY's Code Space">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/41400.html" class="post-title-link" itemprop="url">《HFA-Net：High frequency attention siamese network for building change detection in VHR remote sensing images》笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-11-21 09:41:02" itemprop="dateCreated datePublished" datetime="2022-11-21T09:41:02+08:00">2022-11-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 20:14:36" itemprop="dateModified" datetime="2024-04-03T20:14:36+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>论文地址：<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322001984">HFA-Net: High frequency attention siamese network for building change detection in VHR remote sensing images</a></p>
</blockquote>
<h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2>
<h3 id="动机"><a class="markdownIt-Anchor" href="#动机"></a> 动机</h3>
<p>虽然基于深度学习技术可以很好地处理建筑物变化检测（BCD），但是也存在以下问题：</p>
<p><strong>在对具有更清晰的边界的对象的分割和识别上仍然受高频信息获取不足的影响，导致在建筑物变化检测中建筑物边界的检测效果并不理想。</strong></p>
<h2 id="工作贡献"><a class="markdownIt-Anchor" href="#工作贡献"></a> 工作贡献</h2>
<ul>
<li>提出了一种新的基于孪生网络的框架：<strong>HFA-Net</strong>，用于更好地识别VHR遥感图像中的变化建筑物。</li>
<li>提出了一个<strong>空间注意力（SA）和高频增强（HF）结合的HFAB模块</strong>，首先通过SA引导模型关注建筑物，然后通过HF来突出建筑物的高频部分即边界。HFAB使模型获得更好的特征表示能力。</li>
<li>证实<strong>在深度神经网络中全局高频信息的增强有益于BCD任务</strong>，同时对比之前的基于CNN的方法，该方法能够在BCD任务中达到SOTA效果。</li>
</ul>
<h2 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h2>
<p>网络整体结构如下图所示。该方法采用孪生网络+U-Net结合的架构，其中Encoder部分共享权重，在最下面一层进行concat操作。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121095256549.png" alt="image-20221121095256549" /></p>
<h3 id="hfab"><a class="markdownIt-Anchor" href="#hfab"></a> HFAB</h3>
<p>作者为了实现高频信息的提取的增强，设计了这个HFAB模块。首先通过空间注意力模块给予特征图中建筑物更多的关注，随后通过高频信息增强模块对建筑物边界进行增强。具体网络结构如下图所示。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121095752800.png" alt="image-20221121095752800" /></p>
<p>同时根据HFAB的效果示意图可以很好地理解这一过程，其中先进行空间注意力模块再进行高频信息增强是为了在空间注意力模块中过滤掉一些无关信息，避免HF模块过后输出的特征图包含过多的高频噪声。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121095952361.png" alt="image-20221121095952361" /></p>
<h4 id="sa模块"><a class="markdownIt-Anchor" href="#sa模块"></a> SA模块</h4>
<p>可以在上面流程图中看到，作者采用的空间注意力增强模块时使用了一个U-Net结构作为空间掩码的提取。具体为：上分支部分采用U-Net网络进行处理，而与一般的U-Net不同的是特征通道数一直是降低的，最后输出时通道数为1，便得到了空间掩码<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi><mo stretchy="false">(</mo><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">mask(H \times W \times 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>，然后将它与输入进行相乘实现空间维度的增强</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121100420741.png" alt="image-20221121100420741" /></p>
<h4 id="hf模块"><a class="markdownIt-Anchor" href="#hf模块"></a> HF模块</h4>
<p>作者在提取高频信息的时候采用的是使用Sobel算子。而又因为通常建筑物的形状各异，所以转而使用各向同性的Sobel算子。其中各向同性Sobel算子包含八个方向，于是首先计算每个方向的高频信息，然后进行<strong>求mean</strong>操作。具体操作如下图所示。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121100745080.png" alt="image-20221121100745080" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121100817024.png" alt="image-20221121100817024" /></p>
<p>同时HF模块不仅仅是进行了高频信息的提取，还进行了通道注意力处理。即上分支部分首先进行全局平均池化操作，得到特征图（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">1 \times 1 \times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>），随后经过两个全连接层（FCL）得到通道掩码（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">1 \times 1 \times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>），最后与输入进行通道级别相乘，得到使用通道注意力后的特征。最后与高频分支（即下分支）进行<strong>concat</strong>，然后经过1×1卷积的处理得到最终的特征。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121100439760.png" alt="image-20221121100439760" /></p>
<h3 id="训练细节"><a class="markdownIt-Anchor" href="#训练细节"></a> 训练细节</h3>
<p><strong>Loss</strong>：<strong>BCELOSS</strong></p>
<p>使用Pytorch框架实现了所提出的方法，并在GeForce RTX 3090 GPU和24-GB VRAM上进行了训练。Epoch设置为200，Batchsize设置为8；优化器选用SGD，初始学习率设置为0.01，momentum设置为0.9，weight decay设置为1e-5；学习率在10、15、30、40代时进行衰减，decay rate设置为0.1（MultiStepLR策略）。</p>
<h2 id="实验"><a class="markdownIt-Anchor" href="#实验"></a> 实验</h2>
<h3 id="数据集"><a class="markdownIt-Anchor" href="#数据集"></a> 数据集</h3>
<p><strong>WHU-CD</strong>：航拍图像，空间分辨率为0.075m，尺寸32507×15354</p>
<p><strong>LEVIR-CD</strong>：是一个公共的大规模构建CD数据集。它包含637对大小为1024 × 1024的高分辨率(0.5m) RS图像</p>
<p><strong>Google Dataset</strong>：大规模多光谱卫星图像变化检测数据集，空间分辨率为0.55m。包含19个季节变化的VHR图像对，大小1006×1168到4936×5224不等</p>
<blockquote>
<p><strong>在实验中统一裁剪为256×256大小进行处理</strong></p>
</blockquote>
<h3 id="对比实验"><a class="markdownIt-Anchor" href="#对比实验"></a> 对比实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121101459430.png" alt="image-20221121101459430" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121101511056.png" alt="image-20221121101511056" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121101519444.png" alt="image-20221121101519444" /></p>
<h4 id="可视化比较"><a class="markdownIt-Anchor" href="#可视化比较"></a> 可视化比较</h4>
<p><strong>WHU-CD</strong>：</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121101622113.png" alt="image-20221121101622113" /></p>
<p><strong>LEVIR-CD</strong>：</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121101730948.png" alt="image-20221121101730948" /></p>
<p><strong>Google dataset</strong>：</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121101754453.png" alt="image-20221121101754453" /></p>
<h3 id="消融实验"><a class="markdownIt-Anchor" href="#消融实验"></a> 消融实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121101849808.png" alt="image-20221121101849808" /></p>
<p>可以看出单独的HF模块对方法也有不错的提升，证明高频信息提取和增强处理对变化检测任务是有效的。同时作者还提出SA和HF模块的处理顺序对结果也有影响，先进行SA处理有利于过滤掉不属于建筑物的对象，从而减少输出中的高频噪声。</p>
<h3 id="边界精度评估实验"><a class="markdownIt-Anchor" href="#边界精度评估实验"></a> 边界精度评估实验</h3>
<p>由于提出的方法是为了更好地预测建筑物边界，所以做了相应实验验证提出的方法能够更好地预测变化建筑边界，具体结果如下图所示：</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221121101935623.png" alt="image-20221121101935623" /></p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p><strong>在BCD任务中，针对高频信息进行增强确实有利于结果中边缘的识别</strong>，同时空间注意力和高频信息增强相结合能够一定程度上抑制高频噪声的产生。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://qingy735.github.io/posts/29622.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/title.ico">
      <meta itemprop="name" content="QingY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QingY's Code Space">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | QingY's Code Space">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/29622.html" class="post-title-link" itemprop="url">变化检测-特征差异增强篇（一）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-11-07 18:40:08" itemprop="dateCreated datePublished" datetime="2022-11-07T18:40:08+08:00">2022-11-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 20:14:36" itemprop="dateModified" datetime="2024-04-03T20:14:36+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Summary/" itemprop="url" rel="index"><span itemprop="name">Summary</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="概述"><a class="markdownIt-Anchor" href="#概述"></a> 概述</h2>
<p>目前本人所看的关于变化检测相关的论文，如果按照对特征的处理上来分，本人认为可以分为三种：add、concat、difference。这篇文章将给予对特征差分处理方法做一下展示和总结。</p>
<p>目前主要的框架还是给予U-Net和孪生网络进行实现的，也就是通过提取多尺度多层次特征来实现特征级别上的变化检测。在特征差分方面，同样可以扩展为很多种方法。</p>
<h2 id="论文案例展示"><a class="markdownIt-Anchor" href="#论文案例展示"></a> 论文案例展示</h2>
<h3 id="a-feature-difference-convolutional-neural-network-based-change-detection-method"><a class="markdownIt-Anchor" href="#a-feature-difference-convolutional-neural-network-based-change-detection-method"></a> 《A Feature Difference Convolutional Neural Network-Based Change Detection Method》</h3>
<p>本篇论文在特征提取方面采用了VGG16，一共提取出三个多尺度特征，之后分别对双时相特征进行差分处理，得到差异特征，随后将联合输入图像的特征进行concat，最后经过卷积层的处理得到了变化图。该方法将多尺度特征进行concat，实现了细粒度和粗粒度特征的融合，保证了差异特征的信息丰富度。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221107185406122.png" alt="image-20221107185406122" /></p>
<h3 id="difference-enhancement-and-spatialspectral-nonlocal-network-for-change-detection-in-vhr-remote-sensing-images"><a class="markdownIt-Anchor" href="#difference-enhancement-and-spatialspectral-nonlocal-network-for-change-detection-in-vhr-remote-sensing-images"></a> 《Difference Enhancement and Spatial–Spectral Nonlocal Network for Change Detection in VHR Remote Sensing Images》</h3>
<p>本篇论文主要框架为U-Net+孪生网络。对于差异增强部分既利用上了差异部分，有没有放弃原始特征信息。差异增强模块首先是经过最大池化和平均池化的增强，得到一个差异增强掩码mask（相当于获得一个通道注意力），随后，分别与原始信息进行相乘，得到经过差异增强的特征，最后再做差分操作得到增强差异图。该方法主要想法是首先消除背景和前景的差异表示，减少无关变化对检测结果的影响，然后作用于原始特征，使得背景得到弱化，差异部分得到增强。同时，作者在网络底部使用了一个空间-光谱增强模块，整体结构为非局部注意力，首先利用类似SPPNet的结构对深层特征进行增强，然后进行传统的非局部注意力模块操作进行增强。</p>
<p>实验结论：本文中的差异增强方法和SSN模块能够很好地提高变化检测的准确度，同时上述类似SPPNet的结构对于结果也有不错的改进。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221107185618554.png" alt="image-20221107185618554" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221107185636472.png" alt="image-20221107185636472" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221107185646806.png" alt="image-20221107185646806" /></p>
<h3 id="mdesnet-multitask-difference-enhanced-siamese-network-for-building-change-detection-in-high-resolution-remote-sensing-images"><a class="markdownIt-Anchor" href="#mdesnet-multitask-difference-enhanced-siamese-network-for-building-change-detection-in-high-resolution-remote-sensing-images"></a> 《MDESNet: Multitask Difference-Enhanced Siamese Network for Building Change Detection in High-Resolution Remote Sensing Images》</h3>
<p>这篇文章分为了两部分：变化检测+语义分割。本文将语义分割添加到模型中，希望通过语义分割来约束检测准确率。同时本文的差异特征增强模块在后半部分即语义分割部分。作者认为一般的处理特征分为串联和差分，而两者都有优缺点。特征差分能够很好地展示出变化区域，但是很容易收到非变化目标的影响（光照、季节等），可能包含较多的错误；而串联则能够包含更多的信息，但是却不能有效地展示出变化区域。于是提出了两者相结合的方式来增强差异特征，利用串联的丰富信息知道差分识别出非变换区域，防止受干扰。同时在后续的多尺度特征融合部分采用了层层递进的方式来融合特征，充分利用了各个尺度特征信息。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221108113644095.png" alt="image-20221108113644095" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221108113712080.png" alt="image-20221108113712080" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221108114026220.png" alt="image-20221108114026220" /></p>
<h3 id="smd-net-siamese-multi-scale-difference-enhancement-network-for-change-detection-in-remote-sensing"><a class="markdownIt-Anchor" href="#smd-net-siamese-multi-scale-difference-enhancement-network-for-change-detection-in-remote-sensing"></a> 《SMD-Net: Siamese Multi-Scale Difference-Enhancement Network for Change Detection in Remote Sensing》</h3>
<p>本篇论文主要针对的是大目标边界和小目标检测方面。所以针对深层特征和浅层特征分别做了对应增强。但是整体的架构都是先对差异特征进行增强处理，随后在通道层面上进行串联，保证信息的充分利用。在处理浅层特征上，采用FDM模块进行处理，差异特征经过两个卷积层和一个残差模块，因为浅层特征包含丰富的变化信息，FDM不仅可以增强变化区域特征细节还可以去除一些伪变化的影响。深层特征方面，首先进行最大池化处理，分为几个不同池化大小的特征（进一步扩大感受野），随后经过1*1卷积和上采样得到原特征尺寸的特征图，最后同样进行concat串联输出。该篇论文具体讲解可看之前论文<a target="_blank" rel="noopener" href="https://qingy735.gitee.io/posts/41292.html">《SMD-Net：Siamese Multi-Scale Difference-Enhancement Network for Change Detection in Remote Sensing》笔记 (gitee.io)</a></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221108114555111.png" alt="image-20221108114555111" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221108114609306.png" alt="image-20221108114609306" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221108114617386.png" alt="image-20221108114617386" /></p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>在特征差异增强部分，比较有效的方法：<strong>多核池化（扩大感受野）</strong>、<strong>空间-通道注意力（增强变化区域，削弱伪变化区域）</strong>、<strong>串联-差分结合</strong>、<strong>充分利用特征信息</strong>等</p>
<p>虽然上面提到了几个特征差异方法，但是对于方法的内在原理还不是很明白，后续将在这方面上进行研究。同时上述仅为本人浅陋的想法，如果有不同的想法，欢迎批评指正！</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://qingy735.github.io/posts/36836.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/title.ico">
      <meta itemprop="name" content="QingY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QingY's Code Space">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | QingY's Code Space">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/36836.html" class="post-title-link" itemprop="url">Zotero+OneDrive实现论文自由</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-11-07 16:52:48" itemprop="dateCreated datePublished" datetime="2022-11-07T16:52:48+08:00">2022-11-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 20:14:36" itemprop="dateModified" datetime="2024-04-03T20:14:36+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>本篇文章主要介绍如何在Zotero中使用OneDrive实现备份</p>
</blockquote>
<h2 id="zotero"><a class="markdownIt-Anchor" href="#zotero"></a> Zotero</h2>
<blockquote>
<p>Zotero下载地址<a target="_blank" rel="noopener" href="https://www.zotero.org/download/">Zotero | Downloads</a></p>
</blockquote>
<p>Zotero作为一款非常好用的论文阅读管理器，本人也是一直在使用。但是由于工作电脑和个人笔记本电脑都需要阅读文献，也就衍生出了同步和备份问题，虽然Zotero自带300M空间，但是随着论文下载阅读的增多，突然有一天不能继续备份同步了。所以就发现了可以配合OneDrive来进行备份使用。</p>
<h3 id="zotero网页插件"><a class="markdownIt-Anchor" href="#zotero网页插件"></a> Zotero网页插件</h3>
<h4 id="下载"><a class="markdownIt-Anchor" href="#下载"></a> 下载</h4>
<p>点击上面下载链接，安装浏览器插件即可实现论文自动添加</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221107174231484.png" alt="image-20221107174231484" /></p>
<h4 id="使用说明"><a class="markdownIt-Anchor" href="#使用说明"></a> 使用说明：</h4>
<p>以下载论文《Attention Is All You Need》为例，打开论文页面，会发现浏览器右上角部分会出现类似图标，点击即可选择保存到Zotero什么位置。同时，可以配合插件<em>Sci-Hub</em>进行使用，同步获取到论文pdf文件。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221107174834763.png" alt="image-20221107174834763" /></p>
<h3 id="zotero同步与备份"><a class="markdownIt-Anchor" href="#zotero同步与备份"></a> Zotero同步与备份</h3>
<p>首先可以选择修改Zotero文件存放位置：<strong>编辑</strong>-&gt;<strong>首选项</strong>-&gt;<strong>高级</strong>-&gt;<strong>文件与文件夹</strong>，将下图圈出的部分更改为自己想要存储的位置。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221107175716986.png" alt="image-20221107175716986" /></p>
<p>然后点击<strong>同步</strong>，自己注册账号然后将数据同步全部勾选，数据同步指的是论文中的注释、链接、标签等（除了附件之外的所有内容），同时数据同步是免费和无限制的。当然你也可以将文件同步全部勾选，但是当云端300M空间用完了就没办法继续使用了。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221107180229339.png" alt="image-20221107180229339" /></p>
<h2 id="onedrive"><a class="markdownIt-Anchor" href="#onedrive"></a> OneDrive</h2>
<p>OneDrive是微软公司推出的用来存储文件和照片的云存储空间应用，一般的，个人用户都有5G免费空间可供使用。又因为OneDrive是直接在windows中有存储映射，也就是将文件放入OneDrive文件夹中就可以实现上传备份。所以这里我们可以利用<strong>软链接</strong>的方式实现Zotero存储文件夹和OneDrive文件夹的连接。</p>
<h3 id="建立软链接"><a class="markdownIt-Anchor" href="#建立软链接"></a> 建立软链接</h3>
<p>一般的话Zotero的论文都存储在<strong>Zotero/storage</strong>文件夹中，首先将整个文件夹[storage]移动到OneDrive所在的文件夹中。然后<strong>win+R</strong>打开<strong>cmd</strong>，输入<code>mklink /j [storage原来路径] [OneDrive中storage路径]</code>,回车之后便创建了两个文件夹之间的软链接。切记：一定要先把Zotero中的storage文件夹剪切到OneDrive中，然后再输入上述命令，不然会报错文件夹已存在。创建成功后会发现之前的Zotero文件夹中会有一个新的storage文件夹（左下角有个箭头），类似下图这样，出现这样就代表成功了。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221107181751232.png" alt="image-20221107181751232" /></p>
<h3 id="扩容onedrive"><a class="markdownIt-Anchor" href="#扩容onedrive"></a> 扩容OneDrive</h3>
<p>当然，如果你觉得5G内存还是不够你使用，也还有办法，只要你拥有教育邮箱，就可以先注册一个Office教育账号(<a target="_blank" rel="noopener" href="https://www.microsoft.com/zh-cn/education/products/office">学校和学生免费使用 Microsoft Office 365 | Microsoft 教育</a>)。账号注册完成后就会发现你拥有了一个具有1T的OneDrive账号（如果还不够…)</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://qingy735.github.io/posts/14776.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/title.ico">
      <meta itemprop="name" content="QingY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QingY's Code Space">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | QingY's Code Space">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/14776.html" class="post-title-link" itemprop="url">《Remote Sensing Image Change Detection With Transformers》笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-10-21 09:08:25" itemprop="dateCreated datePublished" datetime="2022-10-21T09:08:25+08:00">2022-10-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 20:14:36" itemprop="dateModified" datetime="2024-04-03T20:14:36+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>原文地址：<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9491802">《Remote Sensing Image Change Detection With Transformers》</a></p>
</blockquote>
<h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2>
<h3 id="问题"><a class="markdownIt-Anchor" href="#问题"></a> 问题</h3>
<p>对于场景中具有复杂目标的情况，高分辨率遥感变化检测仍具有挑战性。具有相同语义概念的物体在不同时间和空间位置上可能表现出截然不同的光谱特征。最近大多数纯CNN模型仍然很难将时空中的长距离概念联系起来，而Nonlocal self-attention方法虽然可以通过建模像素间稠密关系来实现，但是计算效率很低。</p>
<h3 id="工作"><a class="markdownIt-Anchor" href="#工作"></a> 工作</h3>
<blockquote>
<p>提出双时相图像Transformer（BiT）来高效地建模时空域上下文</p>
</blockquote>
<p><strong>核心观点</strong>：感兴趣的高层次概念可以用几个视觉单词来表示，即<strong>语义token</strong></p>
<p>为了实现这一观点，将双时相图像表示为几个语义token，并使用Transformer解码器在紧凑时空中建模上下文。然后将学习到的丰富的上下文token反馈给像素空间，通过Transformer解码器对元是特惠总能进行优化。BiT相比于纯卷积的baseline模型，计算时间和参数量都大大减少。</p>
<h2 id="引言"><a class="markdownIt-Anchor" href="#引言"></a> 引言</h2>
<p><strong>【当前面临的挑战】</strong></p>
<ol>
<li>场景中存在复杂的目标</li>
<li>不同的成像状态</li>
</ol>
<p><strong>【模型需要的能力】</strong></p>
<ol>
<li>识别场景中感兴趣变化的高层语义信息</li>
<li>区分真实变化和伪变化</li>
</ol>
<h3 id="当前任务不足"><a class="markdownIt-Anchor" href="#当前任务不足"></a> 当前任务不足</h3>
<p><strong>【纯卷积】</strong></p>
<p>存在内在的限制，即感受野的大小。通常通过堆叠卷积层或者使用空洞卷积、引入注意力机制来解决。</p>
<p><strong>【注意力机制】</strong></p>
<p>例如通道注意力、空间注意力、self-attention，在对全局的建模中是有效的。但是比较难将时空中的的长范围概念进行联系，主要因为他们要么是将注意力单独应用于每个时态的图像用来增强特征，要么是简单地重加权通道或空间维度融合的双时相特征或者图像中。对于self-attention来说，虽然可以对时空中每一对像素的语义联系进行建模，但是计算量很大。</p>
<h3 id="具体工作"><a class="markdownIt-Anchor" href="#具体工作"></a> 具体工作</h3>
<ol>
<li>使用CNN（ResNet）提取输入图像中的高级语义特征，并且使用空间注意力将每个图像的特征转换为一组紧凑的语义token</li>
<li>使用Transformer编码器对两组语义token进行上下文建模</li>
<li>通过Transformer解码器将富含丰富上下文信息的token映射到像素空间</li>
<li>计算两个特征的差异图并且输入到浅层的CNN中进行像素级的变化预测</li>
</ol>
<h2 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h2>
<p>整体模型结构由三部分组成：CNN特征提取、Transformer、CNN预测，流程图如下所示</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021130108341.png" alt="image-20221021130108341" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021130300735.png" alt="image-20221021130300735" /></p>
<h3 id="semantic-tokenizer"><a class="markdownIt-Anchor" href="#semantic-tokenizer"></a> Semantic Tokenizer</h3>
<p>为了获得紧凑的token，标记器学习一组空间注意力图，将特征图空间池化为一组特征（个人认为可以理解为特征压缩类似，将深度特征在空间维度进行压缩，可以看做是空间注意力的一个变种）</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021130701509.png" alt="image-20221021130701509" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021130738390.png" alt="image-20221021130738390" /></p>
<p>由上面公式可以看出最后输出的语义token维度被压缩为了$L \times C $ ，由后面的实验可以看出<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">L</span></span></span></span>的取值不宜过大。</p>
<h3 id="transformer-encoder"><a class="markdownIt-Anchor" href="#transformer-encoder"></a> Transformer Encoder</h3>
<p>本文中采取了和ViT类似的方法，将Norm层放在了MSA/MLP前面执行，PreNorm已经被证明有更稳定更有能力。而对于输入的两组token（双时相图像）首先进行concat，然后添加位置编码后输入Encoder部分。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021131722563.png" alt="image-20221021131722563" /></p>
<h3 id="transformer-decoder"><a class="markdownIt-Anchor" href="#transformer-decoder"></a> Transformer Decoder</h3>
<p>作者将最初的提取的深层特征作为query，Encoder输出部分作为key和value，同时Decoder部分为一个孪生网络，分别作用于两种图像，输入部分从Encode输出一分为二得到</p>
<h3 id="cnn-backbone"><a class="markdownIt-Anchor" href="#cnn-backbone"></a> CNN Backbone</h3>
<p>使用ResNet18提取双时相图像特征图，同时对最后一层做出了相应修改（详细见原论文）</p>
<h3 id="prediction-head"><a class="markdownIt-Anchor" href="#prediction-head"></a> Prediction Head</h3>
<p>使用一个非常浅的FCN对差异图进行变化预测，即首先通过相减得到两个特征的差异图，之后输入到浅层卷积模块得到通道数为2的变化检测图（分别表示变化or未变化）</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021132745003.png" alt="image-20221021132745003" /></p>
<h3 id="loss-function"><a class="markdownIt-Anchor" href="#loss-function"></a> Loss Function</h3>
<p>使用简单的交叉熵损失进行模型优化</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021132847841.png" alt="image-20221021132847841" /></p>
<h2 id="实验分析"><a class="markdownIt-Anchor" href="#实验分析"></a> 实验分析</h2>
<p>一下为用来比较的模型的详细说明：</p>
<ol>
<li><strong>Base</strong>: our baseline model that consists of the CNN backbone (ResNet18_S5) and the prediction head.</li>
<li><strong>BIT</strong>: our BIT-based model with a light backbone (ResNet18_S4).</li>
<li><strong>Base_S4</strong>: a light CNN backbone (ResNet18_S4) + the prediction head.</li>
<li><strong>Base_S3</strong>: a much light CNN backbone (ResNet18_S3) + the prediction head.</li>
<li><strong>BIT_S3</strong>: our BIT-based model with a much light backbone (ResNet18_S3).</li>
</ol>
<p>在基于<strong>LEVIR-CD</strong>、<strong>WHU-CD</strong>、<strong>DSIFN-CD</strong>数据集上的相关实验如图所示，可以看出BiT在三个数据集上都达到了最好的水平，尤其是在DSIFN-CD数据集上的F1超出其他最好的算法近五个点。同时在Base部分也同样具有很不错的效果，这可能归功于对全局特征的高度抽象，增强了网络的学习能力。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021133246859.png" alt="image-20221021133246859" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021133649615.png" alt="image-20221021133649615" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021133700422.png" alt="image-20221021133700422" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021133706244.png" alt="image-20221021133706244" /></p>
<h3 id="模型参数和计算量"><a class="markdownIt-Anchor" href="#模型参数和计算量"></a> 模型参数和计算量</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021133731563.png" alt="image-20221021133731563" /></p>
<p>可以看出BiT在计算量和参数上面也更占优势，在保持高准确度的同时还保证了参数和计算量很小。</p>
<p>同时可以从下面两张图看出来相比于不使用Transformer而言，BIT模型具有更好的稳定性和泛化能力</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021133913831.png" alt="image-20221021133913831" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021133942528.png" alt="image-20221021133942528" /></p>
<h3 id="消融实验"><a class="markdownIt-Anchor" href="#消融实验"></a> 消融实验</h3>
<p>将深度语义特征提炼为密度更高的语义token能够更好地提升模型性能，使用高度提炼的语义token作为Transformer输入既能减少复杂度又可以减少深度特征中的信息冗余，使得有用的语义信息更加紧凑</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021134103555.png" alt="image-20221021134103555" /></p>
<p>其他消融实验：</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021134233689.png" alt="image-20221021134233689" /></p>
<h3 id="语义token可视化"><a class="markdownIt-Anchor" href="#语义token可视化"></a> 语义token可视化</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021135718537.png" alt="image-20221021135718537" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221021135745032.png" alt="image-20221021135745032" /></p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>本文只使用了比较简单的结构就实现了比较好的效果，其中有几个点值得以后着重思考和实验：</p>
<ol>
<li>语义token，即压缩凝练深层特征使网络学习能力增强</li>
<li>Transformer，Transformer确实可以提高网络的鲁棒性等，并且能够大大增强网络的学习能力</li>
<li>BiT还有很多值得改进的方法，当然也可以直接将他看做是一个Baseline，例如：损失函数（正负样本不均衡问题）、特征提取部分（采用特征提取能力更强的网络）等</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://qingy735.github.io/posts/48279.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/title.ico">
      <meta itemprop="name" content="QingY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QingY's Code Space">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | QingY's Code Space">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/48279.html" class="post-title-link" itemprop="url">目标检测综述（2001-2021）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-10-17 09:47:35" itemprop="dateCreated datePublished" datetime="2022-10-17T09:47:35+08:00">2022-10-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 20:14:36" itemprop="dateModified" datetime="2024-04-03T20:14:36+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Summary/" itemprop="url" rel="index"><span itemprop="name">Summary</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="目标检测综述2001-2021"><a class="markdownIt-Anchor" href="#目标检测综述2001-2021"></a> 目标检测综述（2001-2021）</h1>
<blockquote>
<p>本文参考于文章<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/382702930">目标检测究竟发展到了什么程度？| 目标检测发展22年 - 知乎 (zhihu.com)</a>、<a target="_blank" rel="noopener" href="https://blog.csdn.net/u012655441/article/details/120686525">目标检测综述_RyanC3的博客-CSDN博客_目标检测文献综述</a>、<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/539932517">YOLO家族进化史（v1-v7） - 知乎 (zhihu.com)</a>，编写目的为自身学习方便，不存在转载盈利等！如有问题请及时联系本人。</p>
</blockquote>
<h2 id="背景"><a class="markdownIt-Anchor" href="#背景"></a> 背景</h2>
<p>计算机视觉问题主要分为四个方向：图像分类、图像检测、语义分割和实例分割。作为计算机视觉的基本问题之一，目标检测构成了许多其他视觉任务的基础，例如实例分割、图像标注和目标追踪等；从检测应用的角度看：行人检测、面部检测、文本检测、遥感目标检测统称为目标检测的五大应用。</p>
<h2 id="目标检测算法发展脉络"><a class="markdownIt-Anchor" href="#目标检测算法发展脉络"></a> 目标检测算法发展脉络</h2>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-e0a477f5a1fb362f72123676ef403894_720w.webp" alt="img" /></p>
<h3 id="传统目标检测算法"><a class="markdownIt-Anchor" href="#传统目标检测算法"></a> 传统目标检测算法</h3>
<p>传统的目标检测算法主要基于手工提取特征。传统的检测算法流程如下：</p>
<ol>
<li>选取感兴趣的区域（ROI），选取可能包含物体的区域</li>
<li>对可能包含物体的区域进行特征提取</li>
<li>对提取的特征进行检测分类</li>
</ol>
<h4 id="viola-jones-detector"><a class="markdownIt-Anchor" href="#viola-jones-detector"></a> Viola Jones Detector</h4>
<p><strong>VJ</strong>(Viola Jones)检测器采用滑动窗口的方式以检查目标是否在窗口之中，该检测器看起来似乎很简单稳定，但是由于计算量庞大导致时间复杂度极高，为了解决该项问题，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>J</mi></mrow><annotation encoding="application/x-tex">VJ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span></span></span></span>检测器通过合并三项技术极大地提高了检测速度，三项技术分别为：1）特征的快速计算方法-积分，2）有效的分类器学习方法-AdaBoost，以及3）高效的分类策略-级联结构的设计</p>
<h4 id="hog-detector"><a class="markdownIt-Anchor" href="#hog-detector"></a> HOG Detector</h4>
<p><strong>HOG</strong>（Histogram of Oriented Gradients）检测器于2005年提出，是当时尺度不变性（Scale Invariant Feature Transform)和形状上下文（Shape Contexts）的重要改进，为了平衡特征不变性（包括平移，尺度，光照等）和非线性（区分不同的对象类别），<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi>O</mi><mi>G</mi></mrow><annotation encoding="application/x-tex">HOG</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord mathnormal">G</span></span></span></span>通过在均匀间隔单元的密集网格上计算重叠的局部对比度归一化来提高检测准确率，因此<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi>O</mi><mi>G</mi></mrow><annotation encoding="application/x-tex">HOG</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord mathnormal">G</span></span></span></span>检测器是基于本地像素块进行特征直方图提取的一种算法，它在目标局部变形和受光照影响下都有很好的稳定性。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi>O</mi><mi>G</mi></mrow><annotation encoding="application/x-tex">HOG</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord mathnormal">G</span></span></span></span>为后期很多检测方法奠定了重要基础，相关技术被广泛应用于计算机视觉各大应用。</p>
<h4 id="dpm-detector"><a class="markdownIt-Anchor" href="#dpm-detector"></a> DPM Detector</h4>
<p>作为VOC 2007-2009目标检测挑战赛的冠军，<strong>DPM</strong>（Deformable Parts Model）是目标检测传统算法中当之无愧的SOTA（State Of Art）算法。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>P</mi><mi>M</mi></mrow><annotation encoding="application/x-tex">DPM</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>于2008年提出，相比于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi>O</mi><mi>G</mi></mrow><annotation encoding="application/x-tex">HOG</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord mathnormal">G</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>P</mi><mi>M</mi></mrow><annotation encoding="application/x-tex">DPM</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>做了很多改进，因此该算法可以看作<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi>O</mi><mi>G</mi></mrow><annotation encoding="application/x-tex">HOG</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord mathnormal">G</span></span></span></span>的延伸算法。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>P</mi><mi>M</mi></mrow><annotation encoding="application/x-tex">DPM</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>算法由一个主过滤器（Root-filter）和多个辅过滤器（Part-filters）组成，通过硬负挖掘（Hard negative mining），边框回归（Bounding box regression）和上下文启动（Context priming）技术改进检测精度。作为传统目标检测算法SOTA，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>P</mi><mi>M</mi></mrow><annotation encoding="application/x-tex">DPM</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>方法运行速度快，能够适应物体形变，但它无法适应大幅度的旋转，因此稳定性差。</p>
<h4 id="局限性"><a class="markdownIt-Anchor" href="#局限性"></a> 局限性</h4>
<p>传统的检测算法的一些缺点如下：</p>
<ol>
<li>识别效果不够好，准确率不高</li>
<li>计算量较大，运行速度慢</li>
<li>可能产生多个正确识别的结果</li>
</ol>
<h3 id="anchor-based中的two-stage目标检测算法"><a class="markdownIt-Anchor" href="#anchor-based中的two-stage目标检测算法"></a> Anchor-Based中的Two-stage目标检测算法</h3>
<p>基于手工提取特征的传统目标检测算法进展缓慢，性能低下。直到2012年卷积神经网络（Convolutional Neural Networks，CNNs）的兴起将目标检测推向了新的台阶。基于CNNs的目标检测算法主要有两条技术发展路线：anchor-based和anchor-free方法，而anchor-based方法则包括一阶段和二阶段检测算法（二阶段目标检测算法一般比一阶段精度高，但一阶段检测算法速度会更快）。</p>
<p>Two-stage算法主要分为以下两个阶段：</p>
<p><strong>Stage1</strong>：从图像中生成region proposals</p>
<p><strong>Stage2</strong>：从region proposals生成最终的物体边框</p>
<h4 id="rcnn"><a class="markdownIt-Anchor" href="#rcnn"></a> RCNN</h4>
<blockquote>
<p>论文链接：<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">RCNN</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/rbgirshick/rcnn">https://github.com/rbgirshick/rcnn</a></p>
</blockquote>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-554d6976d29262731db56def37366589_720w.webp" alt="img" /></p>
<p><strong>【简介】</strong> RCNN首先通过选择性搜索算法Selective Search从一组对象候选框中选择可能出现的对象框，然后将这些选择出来的对象框中的图像resize到某一固定尺寸的图像，并输入到CNN模型（经过ImageNet数据集上训练过的CNN模型，如AlexNet）特征提取，最后将提取出的特征送入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>V</mi><mi>M</mi></mrow><annotation encoding="application/x-tex">SVM</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>分类器来预测该对象框中的图像是否存在待检测目标，并进一步预测该检测目标属于哪一类。</p>
<p><strong>【性能】</strong> RCNN算法在VOC-07数据集上取得了非常显著的效果，平均精度由33.7%（DPM-V5，传统检测的SOTA算法）提升到58.5%。相比于传统检测算法，基于深度学习的检测算法在精度上取得了质的飞跃。</p>
<p><strong>【不足】</strong> 重叠框（一张图片大约2000多个候选框）特征的冗余计算使得整个网络的检测速度变得很慢（使用GPU的情况下检测一张图片大约需要14s）。</p>
<p>为了减少大量重叠框带来的冗余计算，K.He等人提出了SPPNet</p>
<h4 id="sppnet"><a class="markdownIt-Anchor" href="#sppnet"></a> SPPNet</h4>
<blockquote>
<p>论文链接：<a target="_blank" rel="noopener" href="https://link.springer.com/content/pdf/10.1007/978-3-319-10578-9_23.pdf">SPPNet</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/yifanjiang97/sppnet-pytorch">GitHub - yifanjiang97/sppnet-pytorch: A simple Spatial Pyramid Pooling layer which could be added in CNN</a></p>
</blockquote>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-43a86774acdaecc0209c8a049601c251_720w.webp" alt="img" /></p>
<p><strong>【简介】</strong> SPPNet提出了一种空间金字塔池化层（Spatial Pyramid Pooling Layer，SPP）。它的主要思路是对于一副图像分成若干尺度的图像块（比如一副图像分成1份，4份，8份等），然后对每一块提取的特征融合在一起，从而兼顾多个尺度的特征。SPP使得网络在全连接层之前能生成固定尺度的特征表示，而不管输入图像尺寸如何。当时用SPP网络进行目标检测时，整个图像只需要计算一次即可生成相应的特征图，不管候选框尺寸如何，经过SPP之后，都能生成固定尺寸的特征表示图，这避免了卷积特征图的重复计算。</p>
<p><strong>【性能】</strong> 相比于RCNN算法，SPPNet在Pascal-07数据集上不牺牲检测精度（VOC-07，mAP=59.2%）的情况下，推理速度提高了20多倍。</p>
<p><strong>【不足】</strong> 与RCNN一样，SPP也需要训练CNN提取特征，然后训练SVM分类这些特征，这需要巨大的存储空间，并且多阶段训练的流程也很繁琐。除此之外，SPPNet只对全连接层进行微调，而忽略了网络其他层的参数。</p>
<p>为了解决以上存在的一些不足，2015年R.Girshick等人提出Fast RCNN</p>
<h4 id="fast-rcnn"><a class="markdownIt-Anchor" href="#fast-rcnn"></a> Fast RCNN</h4>
<blockquote>
<p>论文链接：<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">https://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/rbgirshick/fast-rcnn">rbgirshick/fast-rcnn: Fast R-CNN (github.com)</a></p>
</blockquote>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221017181634870.png" alt="image-20221017181634870" /></p>
<p><strong>【简介】</strong> Fast RCNN是RCNN和SPPNet的改进版，该网络使得我们可以可以在相同的网络配置下同时训练一个检测器和边框回归器。该网络首先输入图像，图像被传递到CNN中提取特征，并返回感兴趣的区域RoI，之后在RoI上运行RoI池化层以保证每个区域的尺寸相同，最后这些区域的特征被传递到全连接层的网络中进行分类，并用Softmax和线性回归层同时返回边界框。</p>
<p><strong>【性能】</strong> Fast RCNN在VOC-07数据集上将检测精度mAP从58.5%提高到70.0%，检测速度比RCNN提高了200倍。</p>
<p><strong>【不足】</strong> FAST RCNN仍然选择用选择性搜索算法来寻找感兴趣的区域，这一过程通常比较慢，与RCNN不同的是，FAST RCNN处理一张图片大约需要2s，但是在大型真实数据集上，这种速度仍然不够理想。</p>
<p>问题来了：可不可以直接使用CNN模型来直接生成候选框？基于此Faster RCNN的提出完美解决了这一问题</p>
<h4 id="faster-rcnn"><a class="markdownIt-Anchor" href="#faster-rcnn"></a> Faster RCNN</h4>
<blockquote>
<p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.01497.pdf">https://arxiv.org/pdf/1506.01497.pdf</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/jwyang/faster-rcnn.pytorch">GitHub - jwyang/faster-rcnn.pytorch: A faster pytorch implementation of faster r-cnn</a></p>
</blockquote>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-5816b6fff1242fb25011a83eb249972a_720w.webp" alt="img" /></p>
<p><strong>【简介】</strong> Faster RCNN是第一个端到端，最接近实时性能的深度学习检测算法，该网络的主要创新点就是提出了区域选择网络用于生成候选框，能极大提升检测框的生成速度。该网络首先输入图像到卷积网络中，生成该图像的特征映射。在特征映射上应用Region Proposal Network，返回object proposals和相应分数。应用RoI池化层，将所有的proposals修正到同样尺寸。最后将proposals传递到全连接层，生成目标物体的边界框。</p>
<p><strong>【性能】</strong> 该网络在当时VOC-07，VOC-12和COCO数据集上实现了SOTA精度，其中COCO mAP@.5=42.7%，COCO mAP@[.5，.95]=21.9%，VOC07 mAP=73.2%，VOC12 mAP=70.4，17fps with ZFNet</p>
<p><strong>【不足】</strong> 虽然Faster RCNN的精度更高，速度更快，也更接近于实时性能，但它在后续的检测阶段中仍存在一些计算冗余；除此之外，如果IOU阈值设置的低，会引起噪声检测的问题，如果IOU设置的高，则会引起过拟合。</p>
<h4 id="fpn"><a class="markdownIt-Anchor" href="#fpn"></a> FPN</h4>
<blockquote>
<p>论文链接：<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf">https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/jwyang/fpn.pytorch">jwyang/fpn.pytorch: Pytorch implementation of Feature Pyramid Network (FPN) for Object Detection (github.com)</a></p>
</blockquote>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-15dd683195074035119181da76d458f1_720w.webp" alt="img" /></p>
<p><strong>【简介】</strong> 2017年，T.-Y.Lin等人在Faster RCNN的基础上进一步提出了特征金字塔网络<strong>FPN</strong>(Feature Pyramid Networks)技术。在FPN技术出现之前，大多数检测算法的检测头都位于网络的最顶层(最深层)，虽说最深层的特征具备更丰富的语义信息，更有利于物体分类，但更深层的特征图由于空间信息的缺乏不利于物体定位，这大大影响了目标检测的定位精度。为了解决这一矛盾，FPN提出了一种具有横向连接的自上而下的网络架构，用于在所有具有不同尺度的高底层都构筑出高级语义信息。FPN的提出极大促进了检测网络精度的提高(尤其是对于一些待检测物体尺度变化大的数据集有非常明显的效果)。</p>
<p><strong>【性能】</strong> 将FPN技术应用于Faster RCNN网络之后，网络的检测精度得到了巨大提高(COCO mAP@.5=59.1%, COCO mAP@[.5,.95]=36.2%)，再次成为当前的SOTA检测算法。此后FPN成为了各大网络(分类，检测与分割)提高精度最重要的技术之一。</p>
<h4 id="cascade-rcnn"><a class="markdownIt-Anchor" href="#cascade-rcnn"></a> Cascade RCNN</h4>
<blockquote>
<p>论文链接：<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf">https://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/zhaoweicai/cascade-rcnn">zhaoweicai/cascade-rcnn: Caffe implementation of multiple popular object detection frameworks (github.com)</a></p>
</blockquote>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-a9c78d59a62d2be6a733657ed0d73695_720w.webp" alt="img" /></p>
<p><strong>【简介】</strong> Faster RCNN完成了对目标候选框的两次预测，其中RPN一次，后面的检测器一次，而<strong>Cascade RCNN</strong>则更进一步将后面检测器部分堆叠了几个级联模块，并采用不同的IOU阈值训练，这种级联版的Faster RCNN就是Cascade RCNN。通过提升IoU阈值训练级联检测器，可以使得检测器的定位精度更高，在更为严格的IoU阈值评估下，Cascade R-CNN带来的性能提升更为明显。Cascade RCNN将二阶段目标检测算法的精度提升到了新的高度。</p>
<p><strong>【性能】</strong> Cascade RCNN在COCO检测数据集上，不添加任何Trick即可超过现有的SOTA单阶段检测器，此外使用任何基于RCNN的二阶段检测器来构建Cascade RCNN，mAP平均可以提高2-4个百分点。</p>
<h3 id="anchor-based中的one-stage目标检测算法"><a class="markdownIt-Anchor" href="#anchor-based中的one-stage目标检测算法"></a> Anchor-based中的one-stage目标检测算法</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/5a63623f9997c8d599cd7726e6960f63.png" alt="img" /></p>
<h4 id="yolo-v1"><a class="markdownIt-Anchor" href="#yolo-v1"></a> YOLO v1</h4>
<blockquote>
<p>论文链接：<a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf">https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/abeardear/pytorch-YOLO-v1">abeardear/pytorch-YOLO-v1: an experiment for yolo-v1, including training and testing. (github.com)</a></p>
</blockquote>
<p><strong>【概述】</strong></p>
<p>以往的二阶段检测算法，如Faster-RCNN，在检测时需要经过两步：边框回归和softmax分类。由于大量预选框的生成，该方法检测精度较高，但实时性较差。</p>
<p>鉴于此，YOLO之父Joseph Redmon创新性的提出了通过直接回归的方式获取目标检测的具体位置信息和类别分类信息，极大的降低了计算量，显著提升了检测的速度，达到了45FPS（Fast YOLO版本达到了155FPS）。</p>
<p><strong>【思路】</strong></p>
<ol>
<li>将输入图片缩放至448×448×3</li>
<li>经过卷积网络backbone提取特征图</li>
<li>将提取到的特征图送入两层全连接层，最终输出7×7×30大小的特征图</li>
</ol>
<p>更进一步讲，就是将输入的图片整体划分为SxS的网格(例如:7x7)，物体中心落在哪一个格子中，那么该格子就负责该物体的检测，每一个格子预测B个边框，输出S×S(B×5+C)。其中5为一个五元组（c,x,y,w,h），分别为confidence置信度，x、y为边框坐标，w、h为边框高和宽；C为class个数，表示为one-hot形式</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-08505cd785f4c5a971d1d7bcf853e1a9_720w.webp" alt="img" /></p>
<p>对于YOLOv1而言，常用的是7x7的网格划分，预测2个边框，输出7x7x30，30个通道包含每个类别的概率+边框置信度+边框位置信息。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-719303fe9618ec3220dc3f382c024775_r.jpg" alt="img" /></p>
<p><strong>【网络结构】</strong></p>
<p>骨干网络：GoogLeNet网络构成：24个卷积层+2个全连接层值得注意的是：YOLOv1版本在第一个卷积层使用的是7x7卷积。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-d6f3a26b1cb6459c4c3db9f781e18012_r.jpg" alt="img" /></p>
<p><strong>【优势和不足】</strong></p>
<p>（1）优势：与二阶段检测算法相比，利用直接回归的方式，大大缩小了计算量，提升了运行速度。</p>
<p>（2）不足：每一个网格仅2个预测框，当存在多物体密集挨着的时候或者小目标的时候，检测效果不好。</p>
<h4 id="yolo-v2"><a class="markdownIt-Anchor" href="#yolo-v2"></a> YOLO v2</h4>
<p><strong>【概述】</strong></p>
<p>针对YOLOv1的问题，YOLO之父Joseph Redmon不甘屈服，对v1版本进行了大刀阔斧的改革，继而提出了YOLOv2网络，重要改革举措包括：</p>
<ol>
<li>
<p>更换骨干网络；</p>
</li>
<li>
<p>引入PassThrough;</p>
</li>
<li>
<p>借鉴二阶段检测的思想，添加了预选框。</p>
</li>
</ol>
<p><strong>【思路】</strong></p>
<p>YOLOv2检测算法是将图片输入到darknet19网络中提取特征图，然后输出目标框类别信息和位置信息。</p>
<p><strong>【网络结构】</strong></p>
<p>骨干网络：Darknet-19，如下图所示（针对1000类别的分类任务）：</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-ce51711358facacb0473b91ed145706a_720w.webp" alt="img" /></p>
<p>只不过对于检测任务而言，需要使用3个3x3卷积（输出通道1024）取代上表中最后的卷积层，再添加passthrough操作后，进行输出。值得注意的是：已不再使用7x7这样的大卷积核。</p>
<p><strong>trick1: PassThrough操作</strong></p>
<p>该方法将28x28x512调整为14x14x2048，后续v5版本中的Focus操作类似该操作。将生成的14x14x2048与原始的14x14x1024进行concat操作。</p>
<p><strong>trick2: 引入anchor，调整位置预测为偏移量预测</strong>借鉴了Faster-RCNN的思想，引入了anchor，将目标框的位置预测由直接预测坐标调整为偏移量预测，大大降低了预测难度，提升了预测准确性。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-ee474e380853c6befe3ef8c0d5ea81fe_r.jpg" alt="img" /></p>
<p><strong>【优势与不足】</strong></p>
<p>（1）优势：利用passthrough操作对高低层语义信息进行融合，在一定程度上增强了小目标的检测能力。采用小卷积核替代7x7大卷积核，降低了计算量。同时改进的位置偏移的策略降低了检测目标框的难度。</p>
<p>（2）尚未采用残差网络结构。且当存在多物体密集挨着的时候或者小目标的时候，检测效果有待提升。</p>
<h4 id="yolo-v3"><a class="markdownIt-Anchor" href="#yolo-v3"></a> YOLO v3</h4>
<p><strong>【概述】</strong></p>
<p>针对YOLOv2的问题，YOLO之父Joseph Redmon决定深化改革。俗话说“他山之石，可以攻玉”，于是乎吸收当下较好的网络设计思想，引入了残差网络模块。重要深化改革的举措：</p>
<ol>
<li>
<p>在darknet19的基础上推陈出新，引入残差，并加深网络深度，提出了Darkent53。</p>
</li>
<li>
<p>借鉴了特征金字塔的思想，在三个不同的尺寸上分别进行预测。</p>
</li>
</ol>
<p><strong>【思路】</strong></p>
<p>YOLOv3检测算法是将图片输入到darknet53网络中提取特征图，然后借鉴特征金字塔网络思想，将高级和低级语义信息进行融合，在低、中、高三个层次上分别预测目标框，最后输出三个尺度的特征图信息（52×52×75、26×26×75、13×13×75）。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-c6dc1783f9bbbf56c947e6ed30f0e554_720w.webp" alt="img" /></p>
<p>其中， 52×52 大小的特征图负责检测小目标， 26×26大小的特征图负责检测中目标， 13×13大小的特征图负责检测大目标。在训练之前，预先通过聚类的方式生成大、中、小三个尺寸的预选框，共9个。预测时最终会输出3x(20+1+4)的数据。一个目标框的输出数据如下：</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-4644e4e71881c135216013c541b94faa_r.jpg" alt="img" /></p>
<p><strong>【网络结构】</strong></p>
<p>骨干网络：Darknet-53</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-bbde78232f75df523732846d4f162121_r.jpg" alt="img" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221017194908833.png" alt="image-20221017194908833" /></p>
<p><strong>tricks:特征金字塔</strong> 该版本借鉴了特征金字塔的思想，只不过与普通的FPN相比略有不同。第一：选择融合的层不同。第二：融合方式不同。对于普通FPN而言，是将高级语义的小尺寸特征图上采样后与上一层进行逐像素相加的融合，融合后尺寸大小和通道数保持不变。而对于YOLOv3而言，是将高级语义的小尺寸特征图上采样到SxS后，选择前面的同为SxS的特征图进行通道方向拼接的融合，融合后，尺寸大小不变但通道数为两者之和。</p>
<p><strong>【优势】</strong></p>
<p>基本解决了小目标检测的问题，在速度和精度上实现了较好的平衡。</p>
<h4 id="yolo-v4"><a class="markdownIt-Anchor" href="#yolo-v4"></a> YOLO v4</h4>
<p><strong>【概述】</strong></p>
<p>Alexey Bochkovskiy对YOLOv3进行了升级改造，核心思想与之前基本一致，不过从数据处理、主干网络、网络训练、激活函数、损失函数等方面对子结构进行了大量的改进。</p>
<p><strong>【思路】</strong></p>
<p>YOLOv4检测算法的基本流程与v3类似，重要升级举措包括：</p>
<ol>
<li>将CSP结构融入Darknet53中，生成了新的主干网络CSPDarkent53</li>
<li>采用SPP空间金字塔池化来扩大感受野</li>
<li>在Neck部分引入PAN结构，即FPN+PAN的形式</li>
<li>引入Mish激活函数</li>
<li>引入Mosaic数据增强</li>
<li>训练时采用CIOU_loss ，同时预测时采用DIOU_nms</li>
</ol>
<p><strong>【网络结构】</strong></p>
<p>骨干网络: CSPDarknet53（含spp）Neck: FPN+PAN</p>
<p>检测头：同v3版本</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-1601febbcce4d1aeea38420342a9548e_720w.webp" alt="img" /></p>
<p><strong>tricks1：输入数据采用Mosaic数据增强</strong></p>
<p>借鉴了2019年CutMix的思路，并在此基础上进行了拓展，Mosaic数据增强方式采用了4张图片，随机缩放、随机裁剪、随机排布的方式进行拼接。从而对小目标的检测起到进一步的提升的作用。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-e286bec1c2402fc3f18c3ae9684829dd_r.jpg" alt="img" /></p>
<p><strong>tricks2：修改骨干网络为</strong>CSPDarknet53</p>
<p>借鉴了2019CSPNet的经验，并结合先前的Darkent53，获得了新的骨干网络CSPDarknet53。在CSPNet中，存在如下操作，即：进入每个stage先将数据划分为两部分，如下图中的part1、part2，区别在于CSPNet中直接对通道维度进行划分，而YOLOv4应用时是利用两个1x1卷积层来实现的。两个分支的信息在交汇处进行Concat拼接。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221017195450370.png" alt="image-20221017195450370" /></p>
<p>**tricks3：**引入spp空间金字塔池化模块</p>
<p>引入SPP结构来增加感受野，采用1x1、5x5、9x9、13x13的最大池化的方式，进行多尺度融合，输出按照通道进行concat融合。类似于语义分割网络PSPNet中的PPM模块。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-806d09570b65a962d31806fa66f0c8ce_r.jpg" alt="img" /></p>
<p><strong>tricks4：在Neck部分采用FPN+PAN的结构</strong></p>
<p>借鉴2018年图像分割领域PANet, 相比于原始的PAN结构，YOLOV4实际采用的PAN结构将addition的方式改为了concatenation。如下图：</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221017195625745.png" alt="image-20221017195625745" /></p>
<p>由于FPN结构是自顶向下的，将高级特征信息以上采样的方式向下传递，但是融合的信息依旧存在不足，因此YOLOv4在FPN之后又添加了PAN结构，再次将信息从底部传递到顶部，如此一来，FPN自顶向下传递强语义信息，而PAN则自底向上传递强定位信息，达到更强的特征聚合效果。整个NECK结构如下图所示：</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-57f52863b72ea01a6392ab1c1d75e848_r.jpg" alt="img" /></p>
<p><strong>【优势】</strong></p>
<p>对比v3和v4版本，在COCO数据集上，同样的FPS等于83左右时，Yolov4的AP是43，而Yolov3是33，直接上涨了10个百分点。</p>
<h4 id="yolo-v5"><a class="markdownIt-Anchor" href="#yolo-v5"></a> YOLO v5</h4>
<p><strong>【简述】</strong></p>
<p>YOLOv5版本 UltralyticsLLC 公司推出的，是在YOLOv4的基础上做了少许的修补，由于改进比较小，仅做简单介绍。改进譬如：</p>
<ol>
<li>
<p>将v4版本骨干网络中的csp结构拓展到了NECK结构中。</p>
</li>
<li>
<p>增加了FOCUS操作，但是后续6.1版本中又剔除掉了该操作，使用一个6x6的卷积进行了替代。</p>
</li>
<li>
<p>使用SPPF结构代替了SPP。</p>
</li>
</ol>
<p><strong>【思路】</strong></p>
<p>YOLOv5检测算法的思路与v4基本一致，此处不再赘述。</p>
<p><strong>【网络结构】</strong></p>
<p>骨干网络: CSPDarknet53（含SPPF）</p>
<p>Neck: FPN+PAN</p>
<p>检测头：同v3版本</p>
<p><strong>tricks1：SPPF</strong></p>
<p>主要区别就是MaxPool由原来的并行调整为了串行，值得注意的是：串行两个 5 x 5 大小的 MaxPool 和一个 9 x 9 大小的 MaxPool 是等价的，串行三个 5 x 5 大小的 MaxPool 层和一个 13 x 13 大小的 MaxPool 是等价的。虽然并行和串行的效果一样，但是串行的效率更高，降低了耗时。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-4b40bb90c2598deee5a16d6df8be240d_r.jpg" alt="img" /></p>
<p><strong>tricks2：自适应锚框计算</strong></p>
<p>比较简单，就是把锚框的聚类改为了使用程序进行自适应计算，此处就不再赘述了。</p>
<p><strong>tricks3：Focus操作</strong> 后续版本剔除了该操作</p>
<p><img src="https://pic3.zhimg.com/80/v2-85281ec1465e9ff7a50c7b0e0ae57ac2_720w.webp" alt="img" /></p>
<h4 id="yolo-v6"><a class="markdownIt-Anchor" href="#yolo-v6"></a> YOLO v6</h4>
<p><strong>【概述】</strong></p>
<p>YOLOv6是由美团推出的，所做的主要工作是为了更加适应GPU设备，将2021年的RepVGG结构引入到了YOLO。思路比较简单，本文仅做少许介绍。</p>
<p><strong>【思路】</strong></p>
<p>YOLOv6检测算法的思路类似YOLOv5（backbone+neck）+YOLOX（head）</p>
<p>主要改动：</p>
<ol>
<li>
<p>骨干网络由CSPDarknet换为了EfficientRep</p>
</li>
<li>
<p>Neck是基于Rep和PAN构建了Rep-PAN</p>
</li>
<li>
<p>检测头部分模仿YOLOX，进行了解耦操作，并进行了少许优化。</p>
</li>
</ol>
<p><strong>【网络结构】</strong></p>
<p>骨干网络: EfficientRep</p>
<p>Neck: FPN+RepPAN</p>
<p>检测头：类似YOLOX</p>
<p><strong>tricks1：引入RepVGG</strong></p>
<p>按照RepVGG的思路，为每一个3x3的卷积添加平行了一个1x1的卷积分支和恒等映射分支，然后在推理时融合为3x3的结构，这种方式对计算密集型的硬件设备会比较友好。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-b1a80269739f17f47cf29c51b9e06e1a_r.jpg" alt="img" /></p>
<p><strong>tricks2：骨干网络EfficientRep</strong></p>
<p>把backbone中stride=2的卷积层换成了stride=2的RepConv层,并且也将CSP-Block修改为了RepBlock。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221018093913212.png" alt="image-20221018093913212" /></p>
<p><strong>tricks3：Neck中引入Rep</strong></p>
<p>为了进一步降低硬件上的耗时，将PAN中的CSP-Block替换为RepBlock, 从而生成了Rep-PAN结构。<img src="https://gitee.com/qingy735/blogimg/raw/master/img/v2-37f89da0300c3c27d720c17f89864169_720w.webp" alt="img" /></p>
<p><strong>tricks4：对检测头解耦并重新设计了高效的解耦头</strong>为了加快收敛速度和降低检测头复杂度，YOLOv6模仿YOLOX对检测头进行了解耦，分开了目标检测中的边框回归过程和类别分类过程。 由于YOLOX的解耦头中，新增了两个额外的3x3卷积，会在一定程度增加运算的复杂度。鉴于此，YOLOv6基于Hybrid Channels的策略重新设计出了一个更高效的解耦头结构。在不怎么改变精度的情况下降低延时，从而达到了速度与精度的权衡。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221018094045527.png" alt="image-20221018094045527" /></p>
<p><strong>【优势】</strong></p>
<p>对耗时做了进一步的优化，进一步提升YOLO检测算法性能。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://qingy735.github.io/posts/53542.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/title.ico">
      <meta itemprop="name" content="QingY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QingY's Code Space">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | QingY's Code Space">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/53542.html" class="post-title-link" itemprop="url">《Deep Multiscale Siamese Network With Parallel Convolutional Structure and Self-Attention for Change Detection》笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-10-11 10:32:35" itemprop="dateCreated datePublished" datetime="2022-10-11T10:32:35+08:00">2022-10-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 20:14:36" itemprop="dateModified" datetime="2024-04-03T20:14:36+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>论文地址：<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9632564">Deep Multiscale Siamese Network With Parallel Convolutional Structure and Self-Attention for Change Detection</a></p>
</blockquote>
<h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2>
<h3 id="动机"><a class="markdownIt-Anchor" href="#动机"></a> 动机</h3>
<p><strong>现有的许多变化检测算法在实际应用中仍需要进一步改进，特别是在增加特征提取的有效性和降低模型计算成本方面。</strong></p>
<p>本文提出的方法在可接受的计算成本下具有出色的特征提取和特征集成能力。该网络主要包含三个子网络：深度多尺度特征提取、PCS进行特征集成、基于self attention特征细化。第一个子网络中设计了一个基于卷积块的深度多尺度孪生网络来描述不同时间图像在不同尺度上的特征；第二个子网络提出了一种PCS模型来综合不同时间图像的多尺度特征；第三个子网络构建SA模型，进一步增强图像信息的表示。</p>
<h3 id="贡献"><a class="markdownIt-Anchor" href="#贡献"></a> 贡献</h3>
<p>作者从特征提取和特征集成的有效性以及降低计算存储成本的角度出发，设计并提出了一种新颖的CD深度神经网络。</p>
<ul>
<li>为了描述多尺度图像特征并提高对不同时间图像的特征提取能力，提出了一种基于设计的卷积块的深度多尺度孪生神经网络（SNN）</li>
<li>为了提升特征融合的有效性，提出了一种并行卷积结构（parallel convolutional structure，PCS）来整合不同时间的特征；此外构建了一个自注意力（SA）模型来进一步提高特征表示能力</li>
<li>提出了一种新的CD模型，在保证检测精度的同时，具有比之前更广的应用范围，大量使用3×3和1×1卷积层来减少参数数量和计算成本</li>
</ul>
<h2 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h2>
<p>整个网络结构分为三部分：共享权重的特征提取模块、基于PCS的特征聚合模块、基于SA的特征细化模块。为了获得图像信息的最优表示，设计了深度多尺度共享权重特征提取器，从不同的时间图像中提取特征。PCS的提出是为了实现不同时相图像的有效特征融合，SA模块为了使特征更具有判别性。网络首先生成多尺度多深度的特征，然后从底向上分别输入PCS和SA模块处理，最后经过卷积层输出变化图。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123190808555.png" alt="image-20221123190808555" /></p>
<h3 id="基于深度多尺度snn的特征提取"><a class="markdownIt-Anchor" href="#基于深度多尺度snn的特征提取"></a> 基于深度多尺度SNN的特征提取</h3>
<p>设计网络的初衷是保证提取特征的有效性和非冗余性，使模型能够以较低的参数量和存储成本实现图像的最优表示。因此，作者提出了一个具有新卷积块和大量3×3和1×1卷积层的深度多尺度SNN。多尺度提取通过引入自适应平均池化操作来实现。通过引入双卷积层和BN层，我们可以在每一层中挖掘更多的特征，这比单卷积层更适合复杂背景下的特征表示。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123192156194.png" alt="image-20221123192156194" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123192210652.png" alt="image-20221123192210652" /></p>
<h3 id="pcs模块"><a class="markdownIt-Anchor" href="#pcs模块"></a> PCS模块</h3>
<p>时态特征融合是CD框架中非常关键的部分，它实现了不同时态图像在特征层面的关联，从而最大限度地提高了融合特征对变化的区分度。因此，时间特征整合的有效性决定了 CD 的性能。本文中提出了一种并行卷积结构（PCS）来完成多尺度特征的集成和图像信息的表示。首先将双时相特征进行concat处理，然后输入PCS模块；PCS主要是分组卷积构成，将卷积分为c组，并且每组的dilation参数设置不同。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123192425473.png" alt="image-20221123192425473" /></p>
<h3 id="sa模块"><a class="markdownIt-Anchor" href="#sa模块"></a> SA模块</h3>
<p>PCS生成的集成特性可以粗略地定位更改，但是它缺乏突出显示区域细节的能力。为了解决这个问题，我们进一步引入了SA模块，我们将 PCS 的输出输入到 SA 中以生成精炼的集成特征。</p>
<h2 id="实验"><a class="markdownIt-Anchor" href="#实验"></a> 实验</h2>
<h3 id="对比实验"><a class="markdownIt-Anchor" href="#对比实验"></a> 对比实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123193345274.png" alt="image-20221123193345274" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123193413102.png" alt="image-20221123193413102" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123193427283.png" alt="image-20221123193427283" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123193448349.png" alt="image-20221123193448349" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123193502622.png" alt="image-20221123193502622" /></p>
<h3 id="消融实验"><a class="markdownIt-Anchor" href="#消融实验"></a> 消融实验</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123193521196.png" alt="image-20221123193521196" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123193535930.png" alt="image-20221123193535930" /></p>
<h3 id="特征可视化"><a class="markdownIt-Anchor" href="#特征可视化"></a> 特征可视化</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221123193610816.png" alt="image-20221123193610816" /></p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>self attention模块处理特征有必要性，同时PCS模块对于特征的聚合有一定的提升效果。</p>
<blockquote>
<p>想要进一步了解可以点击顶部链接查看原论文！</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://qingy735.github.io/posts/28607.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/title.ico">
      <meta itemprop="name" content="QingY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QingY's Code Space">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | QingY's Code Space">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/28607.html" class="post-title-link" itemprop="url">《TransUNetCD：A Hybrid Transformer Network for Change Detection in Optical Remote-Sensing Images》笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-10-10 19:37:26" itemprop="dateCreated datePublished" datetime="2022-10-10T19:37:26+08:00">2022-10-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 20:14:36" itemprop="dateModified" datetime="2024-04-03T20:14:36+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>原文地址：<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9761892/">TransUNetCD: A Hybrid Transformer Network for Change Detection in Optical Remote-Sensing Images</a></p>
</blockquote>
<h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2>
<h3 id="问题"><a class="markdownIt-Anchor" href="#问题"></a> 问题</h3>
<ol>
<li>UNet由于卷积神经网络内在的限制，对全局上下文和长距离的空间联系的获取是不充足的</li>
<li>Transformer虽然能够获取长距离的特征依赖关系，但是缺乏low-level的细节而导致结果中位置信息的缺失</li>
</ol>
<h3 id="工作"><a class="markdownIt-Anchor" href="#工作"></a> 工作</h3>
<blockquote>
<p>提出UNet和Transformer相结合的方法TransUNetCD</p>
</blockquote>
<ol>
<li>对通过卷积神经网络提取的特征进行encoder操作提取丰富的全局上下文信息</li>
<li>经过decoder的连续上采样，通过连续的跳跃连接将其与高分辨率的多尺度特征连接去学习局部-全局语义特征，并恢复特征的全分辨率，实现精确地像素定位</li>
<li>引入差异增强模块，生成包含丰富变化信息的差异图，通过对每个像素的加权和选择性地聚合特征，提高网络的有效性和准确性</li>
</ol>
<h2 id="引言"><a class="markdownIt-Anchor" href="#引言"></a> 引言</h2>
<h3 id="模型设计动机"><a class="markdownIt-Anchor" href="#模型设计动机"></a> 模型设计动机</h3>
<ol>
<li>引入Transformer模块：获取全局上下文信息，提取出更具有代表性的深度特征，减少伪变化的影响</li>
<li>提出差异增强模块DEM：通过对change map进行加权得到更具有判别力的特征表示</li>
</ol>
<h3 id="其他方法不足"><a class="markdownIt-Anchor" href="#其他方法不足"></a> 其他方法不足</h3>
<ul>
<li>RNN：高分辨率图像光谱信息不频繁和时间信息不足</li>
<li>Attention机制（channel、spatial）：只是在channel或者spatial维度增强特征，忽略了浅层特征的增强，不能充分地学习全局上下文并且计算量大</li>
</ul>
<h2 id="研究方法"><a class="markdownIt-Anchor" href="#研究方法"></a> 研究方法</h2>
<h3 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h3>
<p>UNet通过跳跃连接机制实现深层特征和浅层特征的融合，实现特诊提取和细节恢复。将CNN特征提取部分的最后一层进行concat后输入到Transformer中进行high-level特征的进一步增强，获取更具有代表的深度特征，之后经过decoder部分进行连续下采样并与之前的浅层特征进行concat以补充丢失的浅层信息。最后与加权的差异图进行相乘得到最终的变化特征并输出change map</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011092006773.png" alt="image-20221011092006773" /></p>
<h4 id="cascading-upsampling-decodercud"><a class="markdownIt-Anchor" href="#cascading-upsampling-decodercud"></a> Cascading Upsampling Decoder（CUD）</h4>
<p>论文中提到的是由于连续的上采样会丢失low-level细节，所以为了弥补浅层特征的丢失，提出了连续上采样decoder。其本质就是将encoder产生的浅层特征与上采样后的深层特征进行concat后进行一系列卷及操作实现浅层信息的补充</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011092737068.png" alt="image-20221011092737068" /></p>
<h4 id="difference-enhancement-moduledem"><a class="markdownIt-Anchor" href="#difference-enhancement-moduledem"></a> Difference Enhancement Module（DEM）</h4>
<p>双时相图像的差异图能够清晰的展示出图像的变化，但是由于使用了无差异的像素计算，忽略了图像局部特征的差异，也就是不能识别出一些伪变化的情况，所以不能被直接使用。因此本文提出了一种基于SA的差异增强模块，用来识别差异图中的深度特征变化和背景</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011093314509.png" alt="image-20221011093314509" /></p>
<h3 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h3>
<blockquote>
<p>使用简单的Dice loss和加权交叉熵损失<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><msub><mi>L</mi><mrow><mi>w</mi><mi>c</mi><mi>e</mi></mrow></msub><mo>+</mo><msub><mi>L</mi><mrow><mi>d</mi><mi>i</mi><mi>c</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L = L_{wce} + L_{dice}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
</blockquote>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011093450683.png" alt="image-20221011093450683" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011093516106.png" alt="image-20221011093516106" /></p>
<h2 id="实验分析"><a class="markdownIt-Anchor" href="#实验分析"></a> 实验分析</h2>
<h3 id="cdd数据集"><a class="markdownIt-Anchor" href="#cdd数据集"></a> CDD数据集</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011093610319.png" alt="image-20221011093610319" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011093718440.png" alt="image-20221011093718440" /></p>
<h3 id="dsifn数据集"><a class="markdownIt-Anchor" href="#dsifn数据集"></a> DSIFN数据集</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011093807998.png" alt="image-20221011093807998" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011093831369.png" alt="image-20221011093831369" /></p>
<h3 id="levir-cd数据集"><a class="markdownIt-Anchor" href="#levir-cd数据集"></a> LEVIR-CD数据集</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011093858075.png" alt="image-20221011093858075" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011093913833.png" alt="image-20221011093913833" /></p>
<h3 id="whu-cd数据集"><a class="markdownIt-Anchor" href="#whu-cd数据集"></a> WHU-CD数据集</h3>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011093944668.png" alt="image-20221011093944668" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011094002616.png" alt="image-20221011094002616" /></p>
<h3 id="params和flops"><a class="markdownIt-Anchor" href="#params和flops"></a> Params和FLOPs</h3>
<p>从图中可以看出虽然该模型参数量比较高，但是相比于其他方法，计算量并没有增加太多。<em>文中提到参数量太大的原因是使用ViT作为预训练导致的</em></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011094106585.png" alt="image-20221011094106585" /></p>
<h3 id="学习曲线"><a class="markdownIt-Anchor" href="#学习曲线"></a> 学习曲线</h3>
<p>可以看出来该模型相较于BiT方法，收敛速度更快并且具有更好的准确度。同时观察验证集实验可以看出来，该模型的泛化能力更强，具有很好的鲁棒性</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011094458650.png" alt="image-20221011094458650" /></p>
<h3 id="消融实验"><a class="markdownIt-Anchor" href="#消融实验"></a> 消融实验</h3>
<p>从下图中可以看出来在进行特征融合操作中，concat对于结果的提升更加明显，<strong>后续实验可以优先考虑concat</strong>。同时也可以看出来单独的DEM模块对于模型提升并不是很大，但是和Transformer结合后提升效果有所增强，说明经过Transformer处理后的特征具有更好的判别效果。Transformer模块可以聚合全局信息，准确定位变化区域，并且对纹理、形状和大小特征更加鲁棒。DE模块的存在可以提高模型的精度，使模型在前期快速收敛。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011094824489.png" alt="image-20221011094824489" /></p>
<p>从下图对比可以看出来，该方法的鲁棒性比较好，在四倍缩放的情况下依然能够优于BiT。同时观察可视化结果可以看出来该方法对于细长目标的检测更加有效</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011095451796.png" alt="image-20221011095451796" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221011095437782.png" alt="image-20221011095437782" /></p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>1、模型性能好</p>
<p>2、模型的鲁棒性和泛化较于其他模型更加好（个人认为是UNet的跳跃机制导致的，使得深层特征包含更多的浅层信息，同时由于Transformer的存在深层特征更具有代表性，BiT缺少了UNet提供丰富浅层特征的部分）</p>
<p>3、对于有些颜色变化很大的物体变化检测误差比较大（个人认为是由于差异增强模块，单一的依靠像素级别的直接差异判别还是有所弊端，应该修改到特征级别）</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/wps1.jpg" alt="img" /></p>
<p>4、小物体检测性能较差（道路、汽车等）（<strong>浅层特征和深层特征的处理上需要给予更多地关注</strong>，CUD模块似乎并不能很好的处理这方面问题）</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://qingy735.github.io/posts/23553.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/title.ico">
      <meta itemprop="name" content="QingY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QingY's Code Space">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | QingY's Code Space">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/posts/23553.html" class="post-title-link" itemprop="url">《SNUNet-CD：A Densely Connected Siamese Network for Change Detection of VHR Images》笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-10-03 19:00:00" itemprop="dateCreated datePublished" datetime="2022-10-03T19:00:00+08:00">2022-10-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 20:14:36" itemprop="dateModified" datetime="2024-04-03T20:14:36+08:00">2024-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>原文地址：<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9355573">SNUNet-CD: A Densely Connected Siamese Network for Change Detection of VHR Images</a></p>
</blockquote>
<h2 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h2>
<h3 id="问题"><a class="markdownIt-Anchor" href="#问题"></a> 问题：</h3>
<p>一直专注于深层变化语义特征的提取而<strong>忽视了浅层信息的重要性（包含高分辨率和细粒度特征）</strong>，于是经常导致<strong>变化目标边缘像素的不确定性</strong>和<strong>小物体的缺失</strong>。</p>
<h3 id="工作"><a class="markdownIt-Anchor" href="#工作"></a> 工作</h3>
<blockquote>
<p>提出了一个稠密连接孪生网络<strong>SNUNet-CD</strong>来进行变化检测工作</p>
</blockquote>
<p><strong>SNUNet-CD</strong>通过encoder-decoder和decoder-decoder之间紧密的信息传输缓解了深层神经网络位置信息的丢失。并且提出了集成通道注意力模块<strong>ECAM</strong>来实现不同层次的特征的最终提炼。</p>
<h2 id="引言"><a class="markdownIt-Anchor" href="#引言"></a> 引言</h2>
<h3 id="先前工作不足"><a class="markdownIt-Anchor" href="#先前工作不足"></a> 先前工作不足</h3>
<p><strong>基于U-Net的网络结构在连续的下采样后会丢失准确的空间位置信息，这往往会导致变化目标物体的边缘像素不确定和小物体的丢失。</strong></p>
<h3 id="动机"><a class="markdownIt-Anchor" href="#动机"></a> 动机</h3>
<blockquote>
<p>先前的研究已经表明浅层网络包含细粒度的位置信息，深层网络包含更多的粗粒度语义信息。</p>
</blockquote>
<p>基于上述提到的先验知识，可以使用稠密孪生网络作用于变化检测。首先通过encoder获取high-level特征，然后经过decoder时添加浅层特征进行位置信息的补充。同时由于不同层次的特征还是有差距的，所以提出了<strong>ECAM</strong>模块来解决语义鸿沟问题</p>
<p>以下为该论文主要工作和目的：</p>
<ol>
<li>提出基于NestedUNet的稠密连接网络SNUNet-CD：缓解深层网络位置信息丢失</li>
<li>提出ECAM：聚合和细化不同语义层次特征，一直语义鸿沟和定位误差</li>
<li>通过大量实验对比：模型性能（F1-Score）和计算复杂度都优于其他取得SOTA的方法</li>
</ol>
<h2 id="研究方法"><a class="markdownIt-Anchor" href="#研究方法"></a> 研究方法</h2>
<h3 id="网络结构"><a class="markdownIt-Anchor" href="#网络结构"></a> 网络结构</h3>
<p>由于使用孪生网络分别提取双时相图像特征，所以采用concat的方法对特征进行融合来保证信息的完整性。为了保存高分辨率和细粒度的位置信息，该模型在encoder和decoder之间使用dense skip connection机制。如图显示的，在对图像对进行下采样时将两个分支的特征进行融合，并将融合之后的高分辨率、细粒度特征通过跳跃连接传递和相应的decoder部分来实现对深层特征位置信息的补充。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221010183145012.png" alt="image-20221010183145012" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221010183430449.png" alt="image-20221010183430449" /></p>
<h3 id="ecam"><a class="markdownIt-Anchor" href="#ecam"></a> ECAM</h3>
<p>生成的特征图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow></msup><mtext>、</mtext><msup><mi>X</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>2</mn></mrow></msup><mtext>、</mtext><msup><mi>X</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>3</mn></mrow></msup><mtext>、</mtext><msup><mi>X</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">X^{0,1}、X^{0,2}、X^{0,3}、X^{0,4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span>大小是相同的，但是具有不同的语义层次和空间位置表示。具体来说就是浅层输出特征具有更细粒度和精确的位置信息，深层输出特征具有更粗粒度和丰富的语义信息。由于语义鸿沟的存在，必须采用相应的方法来解决融合问题。ECAM通过调整对不同层次特征的关注度来实现对语义鸿沟的抑制（采用Channel Attention，CAM），具体计算流程如下：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mi>A</mi><mi>M</mi><mo stretchy="false">(</mo><mi>F</mi><mo stretchy="false">)</mo><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi>v</mi><mi>g</mi><mi>P</mi><mi>o</mi><mi>l</mi><mi>l</mi><mo stretchy="false">(</mo><mi>F</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo stretchy="false">(</mo><mi>M</mi><mi>a</mi><mi>x</mi><mi>P</mi><mi>o</mi><mi>o</mi><mi>l</mi><mo stretchy="false">(</mo><mi>F</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">CAM(F) = \sigma(MLP(AvgPoll(F)) + MLP(MaxPool(F)))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">o</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>M</mi><mrow><mi>i</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>a</mi></mrow></msub><mo>=</mo><mi>C</mi><mi>A</mi><mi>M</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow></msup><mo>+</mo><msup><mi>x</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>2</mn></mrow></msup><mo>+</mo><msup><mi>x</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>3</mn></mrow></msup><mo>+</mo><msup><mi>x</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>4</mn></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">M_{intra} = CAM(x^{0,1}+x^{0,2}+x^{0,3}+x^{0,4})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.947438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.947438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>F</mi><mrow><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>m</mi><mi>b</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>=</mo><mo stretchy="false">[</mo><msup><mi>x</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow></msup><mo>+</mo><msup><mi>x</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>2</mn></mrow></msup><mo>+</mo><msup><mi>x</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>3</mn></mrow></msup><mo>+</mo><msup><mi>x</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>4</mn></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">F_{ensemble} = [x^{0,1}+x^{0,2}+x^{0,3}+x^{0,4}]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.947438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.947438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>M</mi><mrow><mi>i</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi></mrow></msub><mo>=</mo><mi>C</mi><mi>A</mi><mi>M</mi><mo stretchy="false">(</mo><msub><mi>F</mi><mrow><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>m</mi><mi>b</mi><mi>l</mi><mi>e</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">M_{inter} = CAM(F_{ensemble})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mi>C</mi><mi>A</mi><mi>M</mi><mo stretchy="false">(</mo><msub><mi>F</mi><mrow><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>m</mi><mi>b</mi><mi>l</mi><mi>e</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><msub><mi>F</mi><mrow><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi><mi>m</mi><mi>b</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>+</mo><mi>r</mi><mi>e</mi><mi>p</mi><mi>e</mi><mi>a</mi><msub><mi>t</mi><mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">(</mo><msub><mi>M</mi><mrow><mi>i</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>a</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>⨂</mo><msub><mi>M</mi><mrow><mi>i</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">ECAM(F_{ensemble}) = (F_{ensemble} + repeat_{(4)}(M_{intra})) \bigotimes M_{inter}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.6000100000000002em;vertical-align:-0.55001em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">4</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">⨂</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221010185122202.png" alt="image-20221010185122202" /></p>
<p>最后经过一个1×1卷积输出2×H×W的change map</p>
<h3 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h3>
<blockquote>
<p>采用加权交叉熵损失和dice loss结合</p>
</blockquote>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>=</mo><msub><mi>L</mi><mrow><mi>w</mi><mi>c</mi><mi>e</mi></mrow></msub><mo>+</mo><msub><mi>L</mi><mrow><mi>d</mi><mi>i</mi><mi>c</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L = L_{wce} + L_{dice}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中加权交叉熵损失为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mrow><mi>d</mi><mi>i</mi><mi>c</mi><mi>e</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>H</mi><mo>×</mo><mi>W</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>H</mi><mo>×</mo><mi>W</mi></mrow></munderover><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy="false">[</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo stretchy="false">]</mo><mo separator="true">⋅</mo><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow><mn>1</mn></munderover><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L_{dice} = \frac{1}{H × W} \sum_{k=1}^{H×W}weight[class]·(log(\frac{exp(\hat{y}[k][class])}{\sum_{l=0}^{1}exp(\hat{y}[k][l])}))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.1304490000000005em;vertical-align:-1.302113em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.8478869999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.300005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.302113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mopen">[</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">s</span><span class="mclose">]</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.155992em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.954008em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">]</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">]</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">]</span><span class="mopen">[</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">s</span><span class="mclose">]</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.143718em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<blockquote>
<p>其中class表示0 or 1</p>
</blockquote>
<p>Dice loss：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mrow><mi>d</mi><mi>i</mi><mi>c</mi><mi>e</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mn>2</mn><mo separator="true">⋅</mo><mi>Y</mi><mo separator="true">⋅</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><mrow><mi>Y</mi><mo>+</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">L_{dice} = 1 - \frac{2 · Y · softmax(\hat{Y})}{Y + softmax(\hat{Y})}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.71054em;vertical-align:-1.08677em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.62377em;"><span style="top:-2.16323em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.08677em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<blockquote>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9467699999999999em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>代表change map，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span>表示ground truth</p>
</blockquote>
<h2 id="实验分析"><a class="markdownIt-Anchor" href="#实验分析"></a> 实验分析</h2>
<p>从图中可以看出SNUNet-CD相比于其他的方法能够取得很好的性能，并且在通道数不是很大时参数的大小相较于其他方法也处于不错的位置。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221010190005902.png" alt="image-20221010190005902" /></p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221010190451016.png" alt="image-20221010190451016" /></p>
<p>对于可视化实验分析，可以明显的看出，SNUNet-CD方法能够更加有效地描述变化目标的边缘，如图第三行对于道路的检测；同时对小目标的检测更加有效，如图中第一行车辆部分。</p>
<p><img src="https://gitee.com/qingy735/blogimg/raw/master/img/image-20221010190359114.png" alt="image-20221010190359114" /></p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>该方法采用稠密连接网络的主要出发点是为了弥补多次下采样后浅层信息丢失情况，通过不断的浅层特征的叠加实现最终特征图的信息完整性，这里指大物体边缘信息和小物体自身。实验上也证明了通过特征叠加的有效性，但是可能会出现特征过多导致模型的泛化能力降低，论文中并没有给出相应的实验。同时，本文中提出的解决不同层次语义鸿沟的策略：ECAM，主要是通过通道注意力的方法对浅层和深层特征的关注部分进行自适应性调整后再融合，在上述的消融实验中也证明了ECAM模块的有效性。</p>
<p><strong>核心</strong>：</p>
<ol>
<li>重视浅层特征的主要性，主要为<strong>位置信息</strong>等</li>
<li>对于多尺度特征融合需要进行一定的手段处理，消除或抑制语义鸿沟</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">QingY</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.31/dist/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  





</body>
</html>
